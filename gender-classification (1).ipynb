{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Classification MiniProject\n",
    "\n",
    "This mini-project utilizes the pre-trained EfficientNetB3 model to construct a Convolutional Neural Network (CNN) for gender recognition through images. This is an intriguing application of artificial intelligence in classifying and predicting vital information from image data. The objective of this project is to create an automated system capable of accurately determining the gender of individuals in images.\n",
    "\n",
    "## Step 1: Prepare data\n",
    "\n",
    "**Data source:** https://www.kaggle.com/datasets/ashishjangra27/gender-recognition-200k-images-celeba\n",
    "\n",
    "**Data preprocessing:** Convert photos into digital format and re-size it to 64x64 to fit current configuration and model EfficientNetB3. Only use images from the train folder and test folder. I will mix them all and split them again later with my ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T18:58:10.218240Z",
     "iopub.status.busy": "2023-11-01T18:58:10.217142Z",
     "iopub.status.idle": "2023-11-01T18:58:10.229707Z",
     "shell.execute_reply": "2023-11-01T18:58:10.228397Z",
     "shell.execute_reply.started": "2023-11-01T18:58:10.218200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries for data preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprecessing Female images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T18:58:10.233680Z",
     "iopub.status.busy": "2023-11-01T18:58:10.233343Z",
     "iopub.status.idle": "2023-11-01T18:58:11.971567Z",
     "shell.execute_reply": "2023-11-01T18:58:11.970469Z",
     "shell.execute_reply.started": "2023-11-01T18:58:10.233653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104387"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading image folder path\n",
    "folders = [\n",
    "    \"/kaggle/input/gender-recognition-200k-images-celeba/Dataset/Train/Female\",\n",
    "    \"/kaggle/input/gender-recognition-200k-images-celeba/Dataset/Test/Female\"\n",
    "]\n",
    "\n",
    "female_images_paths = []\n",
    "\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(folder)\n",
    "    sorted_file_names = sorted(file_names, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "    image_paths = [os.path.join(folder, filename) for filename in sorted_file_names]\n",
    "    female_images_paths.extend(image_paths)\n",
    "\n",
    "len(female_images_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T18:58:11.974493Z",
     "iopub.status.busy": "2023-11-01T18:58:11.973758Z",
     "iopub.status.idle": "2023-11-01T19:10:45.211895Z",
     "shell.execute_reply": "2023-11-01T19:10:45.210789Z",
     "shell.execute_reply.started": "2023-11-01T18:58:11.974454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of picture: 104387\n"
     ]
    }
   ],
   "source": [
    "# Convert and re-size images\n",
    "female_data_x = []\n",
    "\n",
    "for image_path in female_images_paths:\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((64, 64))\n",
    "    img = np.array(img, dtype=\"uint8\")\n",
    "    female_data_x.append(img)\n",
    "\n",
    "print(\"number of picture:\", len(female_data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:10:45.213749Z",
     "iopub.status.busy": "2023-11-01T19:10:45.213417Z",
     "iopub.status.idle": "2023-11-01T19:10:45.221291Z",
     "shell.execute_reply": "2023-11-01T19:10:45.220040Z",
     "shell.execute_reply.started": "2023-11-01T19:10:45.213720Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104387"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create labels for images (female will be labeled as 1)\n",
    "female_data_y = np.ones(len(female_data_x))\n",
    "len(female_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprecessing Male images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:10:45.224544Z",
     "iopub.status.busy": "2023-11-01T19:10:45.224149Z",
     "iopub.status.idle": "2023-11-01T19:10:46.683530Z",
     "shell.execute_reply": "2023-11-01T19:10:46.682393Z",
     "shell.execute_reply.started": "2023-11-01T19:10:45.224515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75614"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading image folder path\n",
    "folders = [\n",
    "    \"/kaggle/input/gender-recognition-200k-images-celeba/Dataset/Train/Male\",\n",
    "    \"/kaggle/input/gender-recognition-200k-images-celeba/Dataset/Test/Male\"\n",
    "]\n",
    "\n",
    "male_images_paths = []\n",
    "\n",
    "for folder in folders:\n",
    "    file_names = os.listdir(folder)\n",
    "    sorted_file_names = sorted(file_names, key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "    image_paths = [os.path.join(folder, filename) for filename in sorted_file_names]\n",
    "    male_images_paths.extend(image_paths)\n",
    "\n",
    "len(male_images_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:10:46.685416Z",
     "iopub.status.busy": "2023-11-01T19:10:46.684964Z",
     "iopub.status.idle": "2023-11-01T19:19:42.675556Z",
     "shell.execute_reply": "2023-11-01T19:19:42.674453Z",
     "shell.execute_reply.started": "2023-11-01T19:10:46.685376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of picture: 75614\n"
     ]
    }
   ],
   "source": [
    "# Convert and re-size images\n",
    "male_data_x = []\n",
    "\n",
    "for image_path in male_images_paths:\n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((64, 64))\n",
    "    img = np.array(img, dtype=\"uint8\")\n",
    "    male_data_x.append(img)\n",
    "\n",
    "print(\"number of picture:\", len(male_data_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:19:42.677665Z",
     "iopub.status.busy": "2023-11-01T19:19:42.677243Z",
     "iopub.status.idle": "2023-11-01T19:19:42.685420Z",
     "shell.execute_reply": "2023-11-01T19:19:42.684391Z",
     "shell.execute_reply.started": "2023-11-01T19:19:42.677628Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75614"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create labels for images (male will be labeled as 0)\n",
    "male_data_y = np.zeros(len(male_data_x))\n",
    "len(male_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:19:42.687153Z",
     "iopub.status.busy": "2023-11-01T19:19:42.686799Z",
     "iopub.status.idle": "2023-11-01T19:19:44.302932Z",
     "shell.execute_reply": "2023-11-01T19:19:44.301786Z",
     "shell.execute_reply.started": "2023-11-01T19:19:42.687121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge features data\n",
    "df_x = np.concatenate((female_data_x, male_data_x))\n",
    "len(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:19:44.304615Z",
     "iopub.status.busy": "2023-11-01T19:19:44.304303Z",
     "iopub.status.idle": "2023-11-01T19:19:44.312452Z",
     "shell.execute_reply": "2023-11-01T19:19:44.311331Z",
     "shell.execute_reply.started": "2023-11-01T19:19:44.304589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge labels data\n",
    "df_y = np.concatenate((female_data_y, male_data_y))\n",
    "len(df_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:19:44.314902Z",
     "iopub.status.busy": "2023-11-01T19:19:44.314171Z",
     "iopub.status.idle": "2023-11-01T19:19:46.131234Z",
     "shell.execute_reply": "2023-11-01T19:19:46.130126Z",
     "shell.execute_reply.started": "2023-11-01T19:19:44.314862Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 90% of the data will be used for training, the remaining 10% will be used for testing\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(df_x, df_y, test_size=0.1, random_state=42)\n",
    "\n",
    "print('lenght train_data:', len(train_data))\n",
    "print('lenght val_data:', len(val_data))\n",
    "print('lenght train_labels:', len(train_labels))\n",
    "print('lenght val_labels:', len(val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training model\n",
    "\n",
    "**Pre-trained model:** EfficientNetB3\n",
    "\n",
    "**Optimization algorithm:** Adam\n",
    "\n",
    "**Epoch:** 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:19:46.135734Z",
     "iopub.status.busy": "2023-11-01T19:19:46.135220Z",
     "iopub.status.idle": "2023-11-01T19:20:58.789618Z",
     "shell.execute_reply": "2023-11-01T19:20:58.788493Z",
     "shell.execute_reply.started": "2023-11-01T19:19:46.135691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.9\n",
      "  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.6.3)\n",
      "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9)\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.51.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (3.9.0)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9)\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow==2.9)\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (16.0.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (21.3)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.16.0)\n",
      "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9)\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (0.32.0)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9)\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (4.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.9) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.9) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (2.20.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (3.4.3)\n",
      "Collecting protobuf>=3.9.2 (from tensorflow==2.9)\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9) (2.3.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow==2.9) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (1.26.15)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9) (3.2.2)\n",
      "Installing collected packages: tensorboard-plugin-wit, keras, flatbuffers, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.5.26\n",
      "    Uninstalling flatbuffers-23.5.26:\n",
      "      Successfully uninstalled flatbuffers-23.5.26\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.12.0\n",
      "    Uninstalling tensorflow-estimator-2.12.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.1\n",
      "    Uninstalling tensorboard-data-server-0.7.1:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.3\n",
      "    Uninstalling tensorboard-2.12.3:\n",
      "      Successfully uninstalled tensorboard-2.12.3\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.19.6 which is incompatible.\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n",
      "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.9.0 which is incompatible.\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\n",
      "google-cloud-pubsub 2.17.1 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\n",
      "kfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "onnx 1.14.1 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-decision-forests 1.4.0 requires tensorflow~=2.12.0, but you have tensorflow 2.9.0 which is incompatible.\n",
      "tensorflow-serving-api 2.12.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-serving-api 2.12.1 requires tensorflow<3,>=2.12.0, but you have tensorflow 2.9.0 which is incompatible.\n",
      "tensorflow-text 2.12.1 requires tensorflow<2.13,>=2.12.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed flatbuffers-1.12 google-auth-oauthlib-0.4.6 keras-2.9.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0\n"
     ]
    }
   ],
   "source": [
    "# Run this if this is the first time run this notebook\n",
    "!pip install tensorflow==2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:20:58.791822Z",
     "iopub.status.busy": "2023-11-01T19:20:58.791364Z",
     "iopub.status.idle": "2023-11-01T19:21:03.824817Z",
     "shell.execute_reply": "2023-11-01T19:21:03.823592Z",
     "shell.execute_reply.started": "2023-11-01T19:20:58.791777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries for training model\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageDataGenerator to augment data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:21:03.827031Z",
     "iopub.status.busy": "2023-11-01T19:21:03.826344Z",
     "iopub.status.idle": "2023-11-01T19:21:05.825330Z",
     "shell.execute_reply": "2023-11-01T19:21:05.824272Z",
     "shell.execute_reply.started": "2023-11-01T19:21:03.826996Z"
    }
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "batch_size = 512\n",
    "augmented_data_generator = datagen.flow(train_data, train_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup to save the model with the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:21:05.827258Z",
     "iopub.status.busy": "2023-11-01T19:21:05.826917Z",
     "iopub.status.idle": "2023-11-01T19:21:05.831907Z",
     "shell.execute_reply": "2023-11-01T19:21:05.830921Z",
     "shell.execute_reply.started": "2023-11-01T19:21:05.827223Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model_checkpoint.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build structure for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:21:05.833564Z",
     "iopub.status.busy": "2023-11-01T19:21:05.833206Z",
     "iopub.status.idle": "2023-11-01T19:21:13.369584Z",
     "shell.execute_reply": "2023-11-01T19:21:13.368492Z",
     "shell.execute_reply.started": "2023-11-01T19:21:05.833536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
      "43941136/43941136 [==============================] - 0s 0us/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb3 (Functional)  (None, None, None, 1536)  10783535 \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1536)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               393472    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,210,032\n",
      "Trainable params: 11,122,729\n",
      "Non-trainable params: 87,303\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_model = models.Sequential()\n",
    "\n",
    "base_model = tf.keras.applications.EfficientNetB3(\n",
    "              include_top= False,\n",
    "              weights=\"imagenet\",\n",
    ")\n",
    "\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "pre_model.add(base_model)\n",
    "pre_model.add(layers.GlobalAveragePooling2D())\n",
    "pre_model.add(layers.Flatten())\n",
    "pre_model.add(layers.Dense(256, activation='relu'))\n",
    "pre_model.add(layers.Dropout(0.2))\n",
    "pre_model.add(layers.Dense(128, activation='relu'))\n",
    "pre_model.add(layers.Dropout(0.2))\n",
    "pre_model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "pre_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting data for the model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T19:21:13.371390Z",
     "iopub.status.busy": "2023-11-01T19:21:13.371007Z",
     "iopub.status.idle": "2023-11-01T19:40:41.641887Z",
     "shell.execute_reply": "2023-11-01T19:40:41.640682Z",
     "shell.execute_reply.started": "2023-11-01T19:21:13.371360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "317/317 [==============================] - ETA: 0s - loss: 0.3229 - accuracy: 0.8511\n",
      "Epoch 1: val_accuracy improved from -inf to 0.93456, saving model to best_model_checkpoint.h5\n",
      "317/317 [==============================] - 262s 739ms/step - loss: 0.3229 - accuracy: 0.8511 - val_loss: 0.1728 - val_accuracy: 0.9346\n",
      "Epoch 2/5\n",
      "317/317 [==============================] - ETA: 0s - loss: 0.1685 - accuracy: 0.9322\n",
      "Epoch 2: val_accuracy improved from 0.93456 to 0.94950, saving model to best_model_checkpoint.h5\n",
      "317/317 [==============================] - 225s 710ms/step - loss: 0.1685 - accuracy: 0.9322 - val_loss: 0.1321 - val_accuracy: 0.9495\n",
      "Epoch 3/5\n",
      "317/317 [==============================] - ETA: 0s - loss: 0.1291 - accuracy: 0.9497\n",
      "Epoch 3: val_accuracy improved from 0.94950 to 0.95672, saving model to best_model_checkpoint.h5\n",
      "317/317 [==============================] - 225s 709ms/step - loss: 0.1291 - accuracy: 0.9497 - val_loss: 0.1110 - val_accuracy: 0.9567\n",
      "Epoch 4/5\n",
      "317/317 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9578\n",
      "Epoch 4: val_accuracy improved from 0.95672 to 0.96078, saving model to best_model_checkpoint.h5\n",
      "317/317 [==============================] - 225s 709ms/step - loss: 0.1089 - accuracy: 0.9578 - val_loss: 0.1002 - val_accuracy: 0.9608\n",
      "Epoch 5/5\n",
      "317/317 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9624\n",
      "Epoch 5: val_accuracy improved from 0.96078 to 0.96506, saving model to best_model_checkpoint.h5\n",
      "317/317 [==============================] - 229s 723ms/step - loss: 0.0984 - accuracy: 0.9624 - val_loss: 0.0913 - val_accuracy: 0.9651\n"
     ]
    }
   ],
   "source": [
    "pre_model.compile(optimizer= Adam(learning_rate=0.0001), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "history = pre_model.fit(\n",
    "    augmented_data_generator,\n",
    "    validation_data=(val_data, val_labels),\n",
    "    epochs=5,\n",
    "    batch_size=512,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
